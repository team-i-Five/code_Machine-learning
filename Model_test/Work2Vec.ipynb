{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### konlpy를 사용한 형태소분리와 Word2Vec를 사용한 벡터변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 한국어 전처리를 위한\n",
    "# !pip install konlpy\n",
    "# !pip install Twitter\n",
    "# !pip install tqdm\n",
    "# !pip install gensim\n",
    "# !pip install --upgrade jupyter ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # 운영 체제 관련 작업을 수행하기 위한 Python 표준 라이브러리 중 하나 -> 자바 환경 변수를 설정하기 위해 사용\n",
    "import pandas as pd # 데이터 조작 및 분석을 위한 파이썬 라이브러리 -> 데이터를 데이터프레임으로 로드하고 조작하는 데 사용\n",
    "import numpy as np # 배열 및 수치 연산을 수행하기 위한 라이브러리 -> 다차원 배열 및 수학적 함수를 다루는 데 사용\n",
    "from konlpy.tag import Okt # Konlpy 라이브러리에서 제공하는 한국어 형태소 분석기 중 하나 ->  Okt를 초기화하는 데 사용\n",
    "from gensim.models.word2vec import Word2Vec # Gensim 라이브러리에서 제공하는 Word2Vec 모델을 생성하기 위한 클래스 -> 단어를 고차원 벡터로 표현하여 단어 간 유사성 및 의미 관계를 파악"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 본인 컴퓨터와 자바 설치 경로 설정\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\Program Files\\Java\\jdk-11\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       SEQ_NO  ISBN_THIRTEEN_NO VLM_NM                              TITLE_NM  \\\n",
      "0     6352228     9791156759270    NaN   너에게 목소리를 보낼게  달빛천사 성우 이용신의 첫 번째 에세이   \n",
      "1     6352229     9791168120877    NaN  일기에도 거짓말을 쓰는 사람  99년생 시인의 자의식 과잉 에세이   \n",
      "2     6352230     9791168120839    NaN            본격 한중일 세계사 12  임오군란과 통킹 위기   \n",
      "3     6352231     9791168120846    NaN   즉시 기분을 바꿔드립니다  신기하게 마음이 편해지는 응급 처방전   \n",
      "4     6352232     9791168120747    NaN               오늘도 리추얼  음악 나에게 선물하는 시간   \n",
      "...       ...               ...    ...                                   ...   \n",
      "1997  6354246     9791164670871    NaN                          학교 서클대화가 필요해   \n",
      "1998  6354247     9788994229003    NaN                      People Make City   \n",
      "1999  6354248     9788994027203    NaN                                 동방해경표   \n",
      "2000  6354249     9791156759126    NaN                      큰글자도서 그렇다면 정상입니다   \n",
      "2001  6354250     9788988963258    NaN                                 그 사람들   \n",
      "\n",
      "                  AUTHR_NM PUBLISHER_NM PBLICTE_DE  ADTION_SMBL_NM  PRC_VALUE  \\\n",
      "0                  이용신 지은이          푸른숲        NaN             NaN   160000.0   \n",
      "1                  차도하 지은이       위즈덤하우스        NaN             NaN   158000.0   \n",
      "2                굽시니스트 지은이       위즈덤하우스        NaN             NaN   148000.0   \n",
      "3     올리비아 레메스 지은이 김잔디 옮긴이       위즈덤하우스        NaN             NaN   140000.0   \n",
      "4                  정혜윤 지은이       위즈덤하우스        NaN             NaN   150000.0   \n",
      "...                    ...          ...        ...             ...        ...   \n",
      "1997       손연일 심선화 장경아 지은이          북트리        NaN             NaN   160000.0   \n",
      "1998               이상환 지은이       디자인로커스        NaN             NaN   150000.0   \n",
      "1999                 김려 지음        미디어버스        NaN             NaN        NaN   \n",
      "2000               하지현 지은이          푸른숲        NaN             NaN   360000.0   \n",
      "2001                김규태 지음           말쌈        NaN             NaN        NaN   \n",
      "\n",
      "                                              IMAGE_URL  \\\n",
      "0     httpsimagealadincokrproduct284158coverk6528351...   \n",
      "1     httpsimagealadincokrproduct2841466coverk202835...   \n",
      "2     httpsimagealadincokrproduct2841447coverk402835...   \n",
      "3     httpsimagealadincokrproduct2841430coverk892835...   \n",
      "4     httpsimagealadincokrproduct2841380coverk202835...   \n",
      "...                                                 ...   \n",
      "1997  httpsimagealadincokrproduct2837599coverk262835...   \n",
      "1998  httpsimagealadincokrproduct141865cover89942290...   \n",
      "1999                                                NaN   \n",
      "2000  httpsimagealadincokrproduct2834689coverk292835...   \n",
      "2001                                                NaN   \n",
      "\n",
      "                                         BOOK_INTRCN_CN  KDC_NM  \\\n",
      "0     2004년 방영한 애니메이션 달빛천사에서 주인공 루나풀문 역을 맡으며 90년대생들에...     NaN   \n",
      "1     그러니 나는 말하고 싶은 것을 말하겠다침착하게 사랑하기 차도하 시인 첫 에세이새롭고...     NaN   \n",
      "2     한중일 관계의 결정적 분기점인 임오군란의 막전 막후를 다룬다 러시아의 세력 확장을 ...     NaN   \n",
      "3     누구에게나 기분 구급상자가 필요하다 하나씩 하나씩 차근차근 좋은 기분을 쌓고 건강한...     NaN   \n",
      "4     나다운 일상을 만드는 사람들의 이야기를 담은 오늘도 리추얼 시리즈가 위즈덤하우스에서...     NaN   \n",
      "...                                                 ...     ...   \n",
      "1997  평화로운 학교 공동체의 여정으로 가는 친절한 안내서 관계를 회복하고 배움이 일어나는...     NaN   \n",
      "1998  따뜻한 성북동 만들기피블메이크시티시리즈는 대상지역의 문화 예술적 개발 방법론을 연구...     NaN   \n",
      "1999                                                NaN     NaN   \n",
      "2000  심야 치유 식당 사랑하기에 결코 늦지 않았다 엄마의 빈틈이 아이를 키운다 등을 통해...     NaN   \n",
      "2001                                                NaN     NaN   \n",
      "\n",
      "       TITLE_SBST_NM AUTHR_SBST_NM  TWO_PBLICTE_DE INTNT_BOOKST_BOOK_EXST_AT  \\\n",
      "0                NaN           NaN      20211203.0                       NaN   \n",
      "1                NaN           NaN      20211206.0                       NaN   \n",
      "2                NaN           NaN      20211201.0                       NaN   \n",
      "3                NaN           NaN      20211201.0                       NaN   \n",
      "4                NaN           NaN      20211201.0                       NaN   \n",
      "...              ...           ...             ...                       ...   \n",
      "1997      학교서클대화가필요해  손연일심선화장경아지은이      20211125.0                         Y   \n",
      "1998  peoplemakecity        이상환지은이      20111121.0                         Y   \n",
      "1999           동방해경표          김려지음             NaN                         Y   \n",
      "2000  큰글자도서그렇다면정상입니다        하지현지은이      20211028.0                         Y   \n",
      "2001            그사람들         김규태지음             NaN                         Y   \n",
      "\n",
      "     PORTAL_SITE_BOOK_EXST_AT  ISBN_NO  \n",
      "0                         NaN      NaN  \n",
      "1                         NaN      NaN  \n",
      "2                         NaN      NaN  \n",
      "3                         NaN      NaN  \n",
      "4                         NaN      NaN  \n",
      "...                       ...      ...  \n",
      "1997                        Y      NaN  \n",
      "1998                        Y      NaN  \n",
      "1999                        Y      NaN  \n",
      "2000                        Y      NaN  \n",
      "2001                        Y      NaN  \n",
      "\n",
      "[2002 rows x 18 columns]\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 데이터프레임으로 로드\n",
    "data = pd.read_csv('../cleaned_data.csv', encoding='utf-8')# 데이터 파일 경로\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분리를 위한 Konlpy 객체 초기화\n",
    "twitter = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [2004년, 방영, 한, 애니메이션, 달빛천사, 에서, 주인공, 루나, 풀문, 역...\n",
      "1       [그러니, 나, 는, 말, 하고, 싶은, 것, 을, 말, 하겠다, 침착하게, 사랑,...\n",
      "2       [한중일, 관계, 의, 결정, 적, 분기점, 인, 임오군란, 의, 막전, 막후, 를...\n",
      "3       [누구, 에게나, 기분, 구급상자, 가, 필요하다, 하나, 씩, 하나, 씩, 차근차...\n",
      "4       [나다운, 일상, 을, 만드는, 사람, 들, 의, 이야기, 를, 담은, 오늘, 도,...\n",
      "                              ...                        \n",
      "1997    [평화로운, 학교, 공동체, 의, 여정, 으로, 가는, 친절한, 안내서, 관계, 를...\n",
      "1998    [따뜻한, 성북동, 만들기, 피블, 메이크, 시티, 시리즈, 는, 대상, 지역, 의...\n",
      "1999                                                [nan]\n",
      "2000    [심야, 치유, 식당, 사랑, 하기에, 결코, 늦지, 않았다, 엄마, 의, 빈틈, ...\n",
      "2001                                                [nan]\n",
      "Name: BOOK_INTRCN_CN, Length: 2002, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 각 문서의 형태소 분석을 수행하고, 결과를 리스트에 저장\n",
    "tokenized_data = data['BOOK_INTRCN_CN'].apply(lambda x: twitter.morphs(str(x)))\n",
    "print(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [2004년, 방영, 한, 애니메이션, 달빛천사, 에서, 주인공, 루나, 풀문, 역...\n",
      "1       [그러니, 나, 는, 말, 하고, 싶은, 것, 을, 말, 하겠다, 침착하게, 사랑,...\n",
      "2       [한중일, 관계, 의, 결정, 적, 분기점, 인, 임오군란, 의, 막전, 막후, 를...\n",
      "3       [누구, 에게나, 기분, 구급상자, 가, 필요하다, 하나, 씩, 하나, 씩, 차근차...\n",
      "4       [나다운, 일상, 을, 만드는, 사람, 들, 의, 이야기, 를, 담은, 오늘, 도,...\n",
      "                              ...                        \n",
      "1997    [평화로운, 학교, 공동체, 의, 여정, 으로, 가는, 친절한, 안내서, 관계, 를...\n",
      "1998    [따뜻한, 성북동, 만들기, 피블, 메이크, 시티, 시리즈, 는, 대상, 지역, 의...\n",
      "1999                                                [nan]\n",
      "2000    [심야, 치유, 식당, 사랑, 하기에, 결코, 늦지, 않았다, 엄마, 의, 빈틈, ...\n",
      "2001                                                [nan]\n",
      "Name: BOOK_INTRCN_CN, Length: 2002, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 데이터프레임의 'BOOK_INTRCN_CN' 열에 NaN 값이 있을 때 빈 문자열로 대체\n",
    "tokenized_data = tokenized_data.fillna('')\n",
    "print(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec 모델 훈련을 위한 데이터 준비\n",
    "sentences = data['BOOK_INTRCN_CN'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train과 test dataset 8:2로 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 학습 데이터와 테스트 데이터로 분리\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'BOOK_INTRCN_CN' 열을 선택 후 apply를 사용해 데이터프레임의 각 행에 함수를 적용\n",
    "# str(x)는 각 문서를 문자열로 변환하고 twitter.morphs() 함수는 주어진 문자열을 형태소로 분리 -> 텍스트를 형태소로 분리하는 과정이다.\n",
    "# 그 후 .tolist()메서드를 사용해 형태소 분리된 결과를 리스트로 변환되는데 각 문서가 형태소로 분리된 리스트로 저장된다.\n",
    "# 학습 데이터용 Word2Vec 모델 훈련\n",
    "sentences_train = train_data['BOOK_INTRCN_CN'].apply(lambda x: twitter.morphs(str(x))).tolist()\n",
    "model_train = Word2Vec(sentences=sentences_train, vector_size=100, window=5, min_count=5, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터용 Word2Vec 모델 훈련\n",
    "sentences_test = test_data['BOOK_INTRCN_CN'].apply(lambda x: twitter.morphs(str(x))).tolist()\n",
    "model_test = Word2Vec(sentences=sentences_test, vector_size=30, window=5, min_count=5, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 문서의 벡터 생성\n",
    "def get_doc_vector(doc):\n",
    "    word_vectors = [model.wv[word] for word in doc if word in model.wv]\n",
    "    if not word_vectors:\n",
    "        return np.zeros(model.vector_size)\n",
    "    doc_vector = np.mean(word_vectors, axis=0)\n",
    "    return doc_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 문서의 벡터 생성 (학습 데이터 및 테스트 데이터에 대해 각각)\n",
    "train_data['doc_vector'] = train_data['BOOK_INTRCN_CN'].apply(lambda x: get_doc_vector(twitter.morphs(str(x), model)))\n",
    "test_data['doc_vector'] = test_data['BOOK_INTRCN_CN'].apply(lambda x: get_doc_vector(twitter.morphs(str(x), model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 유사도 검사 -코사인 유사도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_sim_train [[0.99999994 0.99999994 0.99999994 ... 0.99999994 0.99999994 0.14026953]\n",
      " [0.99999994 0.99999994 0.99999994 ... 0.99999994 0.99999994 0.14026953]\n",
      " [0.99999994 0.99999994 0.99999994 ... 0.99999994 0.99999994 0.14026953]\n",
      " ...\n",
      " [0.99999994 0.99999994 0.99999994 ... 0.99999994 0.99999994 0.14026953]\n",
      " [0.99999994 0.99999994 0.99999994 ... 0.99999994 0.99999994 0.14026953]\n",
      " [0.14026953 0.14026953 0.14026953 ... 0.14026953 0.14026953 1.0000001 ]]\n",
      "\n",
      "cosine_sim_test [[0.99999994 0.99999994 0.99999994 ... 0.99999994 0.99999994 0.14026953]\n",
      " [0.14027233 0.14027233 0.14027233 ... 0.14027233 0.14027233 0.99918824]\n",
      " [0.13932896 0.13932896 0.13932896 ... 0.13932896 0.13932896 0.999205  ]\n",
      " ...\n",
      " [0.99999994 0.99999994 0.99999994 ... 0.99999994 0.99999994 0.14026953]\n",
      " [0.13739099 0.13739099 0.13739099 ... 0.13739099 0.13739099 0.99917847]\n",
      " [0.1388085  0.1388085  0.1388085  ... 0.1388085  0.1388085  0.9992235 ]]\n"
     ]
    }
   ],
   "source": [
    "# 유사도 계산 (학습 데이터와 테스트 데이터 간)\n",
    "cosine_sim_train = cosine_similarity(np.stack(train_data['doc_vector']), np.stack(train_data['doc_vector']))\n",
    "cosine_sim_test = cosine_similarity(np.stack(test_data['doc_vector']), np.stack(train_data['doc_vector']))\n",
    "print(\"cosine_sim_train\", cosine_sim_train)\n",
    "print()\n",
    "print(\"cosine_sim_test\",cosine_sim_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유사성을 확인할 문서의 인덱스 선택 (예시: 첫 번째 문서)\n",
    "document_index = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99999994, 0.99999994, 0.99999994, ..., 0.99999994, 0.99999994,\n",
       "       0.14026953], dtype=float32)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities = cosine_sim_test[document_index]  # 해당 테스트 문서와 학습 데이터 간의 유사도\n",
    "similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유사도를 기준으로 내림차순으로 정렬한 후 상위 5개를 추출\n",
    "indices = np.argsort(similarities)[::-1][:5]\n",
    "sorted_similarities = similarities[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제목 #1: 너에게 목소리를 보낼게  달빛천사 성우 이용신의 첫 번째 에세이\n",
      "줄거리: 2004년 방영한 애니메이션 달빛천사에서 주인공 루나풀문 역을 맡으며 90년대생들에게 보석 같은 추억을 선물한 성우 이용신의 첫 번째 에세이 수많은 작품의 주연을 맡으며 쉬지 않고 대중에게 행복을 전해온 성우 이용신의 발자취를 확인할 수 있다\n",
      "유사도: 1.0000\n",
      "\n",
      "제목 #2: 불의 날개와 비밀의 왕국  상\n",
      "줄거리: 전 세계 21개국 출간 뉴욕타임스 베스트셀러 1위  1000만 부 이상 판매 불의 날개 시리즈 제3부 세 번째 이야기에서는 자신의 정체성과 가치를 찾기 위해 정글 부족으로 찾아간 글로리의 이야기가 펼쳐진다\n",
      "유사도: 1.0000\n",
      "\n",
      "제목 #3: 불의 날개와 비밀의 왕국  하\n",
      "줄거리: 독자들을 판타지 세상 파이리아에 푹 빠져들게 할 블록버스터급 대형 판타지 불의 날개 의 제 3부가 출간되었다 세 번째 이야기에서는 자신의 정체성과 가치를 찾기 위해 정글 부족으로 찾아간 글로리의 이야기가 펼쳐진다\n",
      "유사도: 1.0000\n",
      "\n",
      "제목 #4: 1일無식\n",
      "줄거리: 수년간의 과학 연구와 임상 실험 그리고 흥미진진한 이야기들을 통해 우리 몸의 작용 원리와 자기 치유 능력 그리고 그 원리를 돕는 방법들을 소개하고 각종 단식법에 의한 질병 치유의 과정을 알기 쉽게 밝힌다\n",
      "유사도: 1.0000\n",
      "\n",
      "제목 #5: 아무것도 하지 않는 법\n",
      "줄거리: 아무것도 하지 않는 것은 휴대폰을 내려놓고 그 자리에 가만히 머무는 것이다 아무것도 하지 않는 법의 저자 제니 오델은 소셜미디어를 비롯한 관심경제에 사로잡힌 관심의 주권을 되찾아 다른 방향으로 확장하자고 제안한다\n",
      "유사도: 1.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 가장 유사한 문서 5개 출력\n",
    "for i, similar_document_index in enumerate(indices):\n",
    "    similarity = sorted_similarities[i]\n",
    "    similar_title = train_data.loc[similar_document_index, 'TITLE_NM']\n",
    "    similar_doc = train_data.loc[similar_document_index, 'BOOK_INTRCN_CN']\n",
    "    print(f\"제목 #{i + 1}: {similar_title}\")\n",
    "    print(f\"줄거리: {similar_doc}\")\n",
    "    print(f\"유사도: {similarity:.4f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
